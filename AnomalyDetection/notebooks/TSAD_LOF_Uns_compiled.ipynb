{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7d4a4e",
   "metadata": {},
   "source": [
    "# Notebooks suggestions\n",
    "\n",
    "To write these notebooks I've used some of the suggestions in https://towardsdatascience.com/optimizing-jupyter-notebook-tips-tricks-and-nbextensions-26d75d502663, which I found extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7587783",
   "metadata": {},
   "source": [
    "# Interfaces and hierarchical structure\n",
    "\n",
    "The structure of the classes has been designed to be hierarchical to follow the DRY principle. To do so, whenever some code can be reused from different classes, inheritance and appropriate design patterns are used. Interfaces are a key component definining objects in an OOP programs. Every exposed interface of an object is defined and then implemented by objects. Moreover, I use interfaces to define common behaviours more than simple exposed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02200a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class IClassifier(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def classify(self, X, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class IRegressor(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def regress(self, x, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class IParametric(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, x, y=None, *args, **kwargs) -> None:\n",
    "        pass\n",
    "    \n",
    "class IAnomalyRegressor(IRegressor):\n",
    "    @abc.abstractmethod\n",
    "    def anomaly_score(self, x, *args, **kwargs) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "class IAnomalyClassifier(IClassifier, ABC):\n",
    "    pass\n",
    "\n",
    "class ITimeSeriesAnomaly(IAnomalyRegressor, IAnomalyClassifier, ABC):\n",
    "    pass\n",
    "\n",
    "class ITimeSeriesAnomalyWindow(ITimeSeriesAnomaly):\n",
    "    @abc.abstractmethod\n",
    "    def _project_time_series(self, time_series: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _compute_point_scores(self, window_scores,\n",
    "                              windows_per_point) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _compute_point_labels(self, window_labels,\n",
    "                              windows_per_point,\n",
    "                              point_scores=None) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _compute_window_scores(self, vector_data: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def _compute_window_labels(self, vector_data: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class ITimeSeriesAnomalyWrapper(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def _build_wrapped(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7423f",
   "metadata": {},
   "source": [
    "# Interfaces for data readers\n",
    "\n",
    "Since data must be read from file, all types of data readers share some properties: they read from file, they return the dataset as a dataframe and they perform splitting of the dataset. Therefore, here it comes the need of abstracting interfaces for such classes. The interfaces define methods which will be public or at most protected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcd0f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import abc\n",
    "from abc import ABC\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class IDataReader(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def read(self, path: str,\n",
    "             file_format: str = \"csv\") -> IDataReader:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_dataframe(self) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "class IDataSupervised(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def get_ground_truth(self, col_name: str) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "class IDataTrainTestSplitter(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def train_test_split(self, train_perc: float = 0.8) -> IDataTrainTestSplitter:\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_train_test_dataframes(self) -> Tuple[pd.DataFrame]:\n",
    "        pass\n",
    "\n",
    "class IDataTrainValidTestSplitter(ABC):\n",
    "    @abc.abstractmethod\n",
    "    def train_valid_test_split(self, train_perc: float = 0.7,\n",
    "                               valid_perc: float = 0.1) -> IDataTrainValidTestSplitter:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_train_valid_test_dataframes(self) -> Tuple[pd.DataFrame]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc3776",
   "metadata": {},
   "source": [
    "# Input validation procedures\n",
    "\n",
    "Input to many methods must be validated in some ways. Moreover, a lot of methods must perform the exact same checks on the input. Therefore, input validation procedures are extracted and written separately out of classes. Then, these methods are used to validate input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d2d9a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "def check_array_1d(X, array_name: str = None) -> None:\n",
    "    check_array(X, ensure_2d=False)\n",
    "\n",
    "    array_name = array_name if array_name is not None else \"X\"\n",
    "    X = np.array(X)\n",
    "\n",
    "    if X.ndim > 1:\n",
    "        raise ValueError(array_name + \" must be 1 dimensional array\")\n",
    "\n",
    "\n",
    "def check_x_y_smaller_1d(X, y, x_name: str = None, y_name: str = None):\n",
    "    check_array_1d(X, array_name=x_name)\n",
    "    check_array_1d(y, array_name=y_name)\n",
    "\n",
    "    x_name = x_name if x_name is not None else \"X\"\n",
    "    y_name = y_name if y_name is not None else \"y\"\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if y.size < X.size:\n",
    "        raise ValueError(x_name + \" cannot have more elements than \" + y_name)\n",
    "\n",
    "def check_attributes_exists(estimator,\n",
    "                            attributes: Union[str, list[str]]) -> None:\n",
    "    if isinstance(attributes, str):\n",
    "        if attributes not in estimator.__dict__.keys():\n",
    "            raise ValueError(\"%s does not have attribute %s\" %\n",
    "                             (estimator.__class__, attributes))\n",
    "    else:\n",
    "        for attribute in attributes:\n",
    "            if attribute not in estimator.__dict__.keys():\n",
    "                raise ValueError(\"%s does not have attribute %s\" %\n",
    "                                 (estimator.__class__, attribute))\n",
    "\n",
    "def check_not_default_attributes(estimator,\n",
    "                                 attributes: dict) -> None:\n",
    "    for key, value in attributes.items():\n",
    "        check_attributes_exists(estimator, key)\n",
    "        attr_val = getattr(estimator, key)\n",
    "        if value is None:\n",
    "            if attr_val is None:\n",
    "                raise RuntimeError(\"Train the model before calling this method\")\n",
    "        else:\n",
    "            if attr_val == value:\n",
    "                raise RuntimeError(\"Train the model before calling this method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3755e36",
   "metadata": {},
   "source": [
    "# General data reader\n",
    "\n",
    "Given that we have all the interfaces to implement a data reader, we need to implement a general data reader capable of reading datasets. To be as general as possible, this data reader should only perform simple operations such as reading a dataset whose form is not known, split it in training and testing or training, validation, testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbfa01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TimeSeriesReader(IDataReader,\n",
    "                       IDataSupervised,\n",
    "                       IDataTrainTestSplitter,\n",
    "                       IDataTrainValidTestSplitter):\n",
    "    ACCEPTED_FORMATS = [\"csv\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.path : str = \"\"\n",
    "        self.format : str = \"\"\n",
    "        self.dataset : pd.DataFrame = None\n",
    "        self.train_frame : pd.DataFrame = None\n",
    "        self.valid_frame : pd.DataFrame = None\n",
    "        self.test_frame : pd.DataFrame = None\n",
    "    \n",
    "    def read(self, path: str,\n",
    "             file_format: str = \"csv\") -> TimeSeriesReader:\n",
    "        if file_format not in self.ACCEPTED_FORMATS:\n",
    "            raise ValueError(\"The file format must be one of %s\" %\n",
    "                             self.ACCEPTED_FORMATS)\n",
    "        elif path == \"\":\n",
    "            raise ValueError(\"The path cannot be empty\")\n",
    "        \n",
    "        self.path = path\n",
    "        self.format = file_format\n",
    "        \n",
    "        self.dataset = pd.read_csv(self.path)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train_test_split(self, train_perc: float = 0.8) -> TimeSeriesReader:\n",
    "        check_not_default_attributes(self, {\"dataset\": None})\n",
    "        \n",
    "        if not 0 < train_perc < 1:\n",
    "            raise ValueError(\"The training percentage must lie in (0,1) range.\")\n",
    "        \n",
    "        num_of_test = int((1 - train_perc) * self.dataset.shape[0])\n",
    "        self.train_frame = self.dataset[0:-num_of_test]\n",
    "        self.test_frame = self.dataset[-num_of_test:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train_valid_test_split(self, train_perc: float = 0.7,\n",
    "                               valid_perc: float = 0.1) -> TimeSeriesReader:\n",
    "        check_not_default_attributes(self, {\"dataset\": None})\n",
    "        \n",
    "        if not 0 < train_perc < 1 or not 0 < valid_perc < 1:\n",
    "            raise ValueError(\"Training and validation percentages must lie in \"\n",
    "                             \"(0,1) range.\")\n",
    "        elif train_perc + valid_perc >= 1:\n",
    "            raise ValueError(\"Training and validation must be less than all the \"\n",
    "                             \"dataset, i.e., their sum lies in (0,1).\")\n",
    "        \n",
    "        num_of_not_train = int((1 - train_perc) * self.dataset.shape[0])\n",
    "        num_of_test = int((1 - train_perc - valid_perc) * self.dataset.shape[0])\n",
    "        self.train_frame = self.dataset[0:-num_of_not_train]\n",
    "        self.valid_frame = self.dataset[-num_of_not_train:-num_of_test]\n",
    "        self.test_frame = self.dataset[-num_of_test:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_dataframe(self) -> pd.DataFrame:\n",
    "        check_not_default_attributes(self, {\"dataset\": None})\n",
    "        return self.dataset.copy()\n",
    "    \n",
    "    def get_train_test_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        check_not_default_attributes(self, {\"train_frame\": None,\n",
    "                                            \"test_frame\": None})\n",
    "        return self.train_frame.copy(), self.test_frame.copy()\n",
    "    \n",
    "    def get_train_valid_test_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        check_not_default_attributes(self, {\"train_frame\": None,\n",
    "                                            \"valid_frame\": None,\n",
    "                                            \"test_frame\": None})\n",
    "        return self.train_frame.copy(), self.valid_frame.copy(), self.test_frame.copy()\n",
    "    \n",
    "    def get_ground_truth(self, col_name: str) -> np.ndarray:\n",
    "        check_not_default_attributes(self, {\"dataset\": None})\n",
    "        \n",
    "        if col_name not in self.dataset.columns:\n",
    "            raise ValueError(\"The column specified does not exist\")\n",
    "        \n",
    "        targets = self.dataset[col_name]\n",
    "        return np.array(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e1658",
   "metadata": {},
   "source": [
    "# NAB data reader\n",
    "\n",
    "NAB is a known dataset to perform anomaly detection. Datasets are not specified in a single csv file. Time series raw data are stored in a file, then, labels and windows are stored in separate files as json structure resembling that of a simple dictionary. Therefore, we need to slightly change the code of the read function of the general reader to be able to correctly read NAB datasets to be able to correctly identify the labels for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07767c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NABTimeSeriesReader(TimeSeriesReader):\n",
    "    def __init__(self, label_path: str,\n",
    "                 labels_name: str = \"combined_labels.json\",\n",
    "                 window_name: str = \"combined_windows.json\",\n",
    "                 save_windows: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_path = label_path\n",
    "        self.labels_name = labels_name\n",
    "        self.window_name = window_name\n",
    "        self.save_windows = save_windows\n",
    "        self.combined_labels = {}\n",
    "        self.combined_windows = {}\n",
    "\n",
    "    def read(self, path: str,\n",
    "             file_format: str = \"csv\") -> NABTimeSeriesReader:\n",
    "        if \"/\" in path:\n",
    "            sep = \"/\"\n",
    "        else:\n",
    "            sep = \"\\\\\"\n",
    "\n",
    "        # Get the dataset filename\n",
    "        dataset_file = path.split(sep)[-1]\n",
    "        \n",
    "        # Gets the dictionaries of combined labels and windows\n",
    "        file = open(self.label_path + self.labels_name)\n",
    "        self.combined_labels : dict = json.load(file)\n",
    "        file.close()\n",
    "        file = open(self.label_path + self.window_name)\n",
    "        self.combined_windows : dict = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        # Gets the key of the dataset\n",
    "        dataset_key = list(filter(lambda x: dataset_file in x, self.combined_labels.keys()))\n",
    "        dataset_key = dataset_key[0]\n",
    "        \n",
    "        # Get the ground truth of the desired dataset\n",
    "        windows = self.combined_windows[dataset_key]\n",
    "        labels = self.combined_labels[dataset_key]\n",
    "        \n",
    "        # Generate the dataset with ground truth\n",
    "        super().read(path=path, file_format=file_format)\n",
    "        timestamps = self.dataset[\"timestamp\"]\n",
    "        ground_truth = [0] * self.dataset.shape[0]\n",
    "        for i in range(self.dataset.shape[0]):\n",
    "            timestamp = datetime.datetime.strptime(timestamps[i],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            if self.save_windows:\n",
    "                for window in windows:\n",
    "                    first = datetime.datetime.strptime(window[0],\n",
    "                                                       \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                    last = datetime.datetime.strptime(window[1],\n",
    "                                                      \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                    if first <= timestamp <= last:\n",
    "                        ground_truth[i] = 2\n",
    "                        break\n",
    "            \n",
    "            if timestamps[i] in labels:\n",
    "                ground_truth[i] = 1\n",
    "        \n",
    "        # Convert into numpy array the ground truth and add it to the dataframe\n",
    "        truth = np.array(ground_truth)\n",
    "        self.dataset.insert(len(self.dataset.columns),\n",
    "                            \"target\",\n",
    "                            truth)\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1c6da",
   "metadata": {},
   "source": [
    "# Base model\n",
    "\n",
    "Every model learning concepts and performing operations on data in input is characterized by some parameters. Therefore, every model needs a method to set the parameters and a method to get the parameters. A basic and simple definition of a base model object is implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b5a1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def set_params(self, **params) -> None:\n",
    "        for key, value in params.items():\n",
    "            if key not in self.__dict__.keys():\n",
    "                raise ValueError(\"Parameter '%s' does not exist in class '%s'. \"\n",
    "                                 \"Please, read either the signature or the \"\n",
    "                                 \"docs for that class.\" %\n",
    "                                 (key, self.__class__.__name__))\n",
    "            else:\n",
    "                self.__dict__[key] = value\n",
    "                \n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        init = getattr(self, \"__init__\")\n",
    "        if init is object.__init__:\n",
    "            return {}\n",
    "        \n",
    "        init_signature = inspect.signature(init)\n",
    "        parameters = [p\n",
    "                      for p in init_signature.parameters.values()\n",
    "                      if p.name != \"self\" and p.kind == p.POSITIONAL_OR_KEYWORD]\n",
    "        parameters_name = [name.name\n",
    "                           for name in parameters]\n",
    "        params_to_return = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key in parameters_name:\n",
    "                params_to_return[key] = value\n",
    "                \n",
    "        return params_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73573598",
   "metadata": {},
   "source": [
    "# Approaches using sliding window in univariate time series\n",
    "\n",
    "Univariate time series are characterized by a time ordered sequence of values, which in general are real numbers. Therefore, to be able to use spatial machine learning algorithms to perform anomaly detection, we need an approach to rearrange data to treat them as if they were spatial data. To do so, we define two parameters:\n",
    "\n",
    "- Window: it is the number of consecutive samples that will be grouped together into a vector of window data. It must be at least 1.\n",
    "- Stride: it is the number of steps by which we move the window. It must be at least 1.\n",
    "\n",
    "Now, given that we have defined what sliding window methods works in general, we can even describe how anomalies are computed. If `stride < window`, several points will be used to create multiple windows, e.g., with `window = 3` and `stride = 1`, the point at index `i = 2` will be contained in three windows: the first, the second and the third. Therefore, once we compute the scores and the labels of windows, we need a way to come back to the time series space by scoring and labelling points of the time series. Because of that, we distinguish two ways to perform labelling of points and one way to compute the score of the points. The scoring methods are:\n",
    "\n",
    "- Average: for each point we take all the windows containing this point. Then, we average the scores of the windows and we give this score to the point.\n",
    "\n",
    "Moreover, score can be rescaled with the following options:\n",
    "\n",
    "- None: scores are not normalized.\n",
    "- MinMax normalization: scores are normalized using MinMax normalization implemented in `scikit-learn`.\n",
    "\n",
    "The labelling methods are:\n",
    "\n",
    "- Voting: given a threshold $\\tau$ of agreement between windows built using a point $p$, if at least $\\tau$ percentage of windows agree that the point is an anomaly, it will be classified as anomaly. Otherwise, the point is considered normal.\n",
    "- Points score: given the scores of points, we compute the mean and standard deviation to fit a truncated gaussian of the scores. Then, each point having a score greater than the qth-q quantile specified is classified as anomaly. I.e., if we want to compute the qth-q quantile 0.999, every point whose probability is less than 0.001 will be classified as anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b67f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "class TimeSeriesAnomalyWindow(ITimeSeriesAnomalyWindow, BaseModel, ABC):\n",
    "    ACCEPTED_SCORING_METHODS = [\"average\"]\n",
    "    ACCEPTED_SCALING_METHODS = [\"none\", \"minmax\"]\n",
    "    ACCEPTED_LABELLING_METHODS = [\"voting\", \"points_score\"]\n",
    "    \n",
    "    def __init__(self, window: int = 5,\n",
    "                 stride: int = 1,\n",
    "                 scaling: str = \"minmax\",\n",
    "                 scoring: str = \"average\",\n",
    "                 classification: str = \"voting\",\n",
    "                 threshold: float = None,\n",
    "                 anomaly_portion: float = 0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.window = window\n",
    "        self.stride = stride\n",
    "        self.scaling = scaling\n",
    "        self.scoring = scoring\n",
    "        self.classification = classification\n",
    "        self.threshold = threshold\n",
    "        self.anomaly_portion = anomaly_portion\n",
    "        \n",
    "        self.__check_parameters()\n",
    "    \n",
    "    def set_params(self, **params) -> None:\n",
    "        super().set_params()\n",
    "        self.__check_parameters()\n",
    "    \n",
    "    def _project_time_series(self, time_series: np.ndarray) -> Tuple[np.ndarray,\n",
    "                                                                     np.ndarray]:\n",
    "        # Input validation\n",
    "        check_array(time_series)\n",
    "        data = np.array(time_series)\n",
    "\n",
    "        if self.window > data.shape[0]:\n",
    "            raise ValueError(\"Window cannot be larger than data size.\")\n",
    "        elif data.shape[1] > 1:\n",
    "            raise ValueError(\"Only univariate time series is currently \"\n",
    "                             \"supported.\")\n",
    "        elif (data.shape[0] - self.window) % self.stride != 0:\n",
    "            raise ValueError(\"Data.shape[0] - window must be a multiple of \"\n",
    "                             \"stride to build the spatial data.\")\n",
    "\n",
    "        # Number of times a point is considered in a window\n",
    "        num_windows = np.zeros(data.shape[0])\n",
    "        x_new = []\n",
    "\n",
    "        # Transform univariate time series into spatial data\n",
    "        for i in range(0, data.shape[0] - self.window + 1, self.stride):\n",
    "            num_windows[i:i + self.window] += 1\n",
    "            current_data: np.ndarray = data[i:i + self.window]\n",
    "            current_data = current_data.reshape(current_data.shape[0])\n",
    "            x_new.append(current_data.tolist())\n",
    "\n",
    "        x_new = np.array(x_new)\n",
    "\n",
    "        return x_new, num_windows\n",
    "    \n",
    "    def _compute_point_scores(self, window_scores,\n",
    "                              windows_per_point) -> np.ndarray:\n",
    "        check_x_y_smaller_1d(window_scores, windows_per_point)\n",
    "\n",
    "        window_scores = np.array(window_scores)\n",
    "        windows_per_point = np.array(windows_per_point)\n",
    "\n",
    "        # Compute score of each point\n",
    "        scores = np.zeros(windows_per_point.shape[0])\n",
    "        for i in range(window_scores.shape[0]):\n",
    "            idx = i * self.stride\n",
    "            scores[idx:idx + self.window] += window_scores[i]\n",
    "\n",
    "        match self.scoring:\n",
    "            case \"average\":\n",
    "                scores = scores / windows_per_point\n",
    "\n",
    "        match self.scaling:\n",
    "            case \"minmax\":\n",
    "                # Min-max normalization\n",
    "                scores = scores.reshape((scores.shape[0], 1))\n",
    "                scores = MinMaxScaler().fit_transform(scores)\n",
    "                scores = scores.reshape(scores.shape[0])\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def _compute_point_labels(self, window_labels,\n",
    "                              windows_per_point,\n",
    "                              point_scores=None) -> Tuple[np.ndarray, float]:\n",
    "        check_x_y_smaller_1d(window_labels, windows_per_point)\n",
    "\n",
    "        window_labels = np.array(window_labels)\n",
    "        windows_per_point = np.array(windows_per_point)\n",
    "\n",
    "        threshold = self.threshold\n",
    "        labels = np.zeros(windows_per_point.shape[0])\n",
    "        match self.classification:\n",
    "            case \"voting\":\n",
    "                # Anomalies are computed by voting of window anomalies\n",
    "                for i in range(window_labels.shape[0]):\n",
    "                    if window_labels[i] == 1:\n",
    "                        idx = i * self.stride\n",
    "                        labels[idx:idx + self.window] += 1\n",
    "                labels = labels / windows_per_point\n",
    "\n",
    "                if threshold is None:\n",
    "                    threshold = 0.5\n",
    "\n",
    "                true_anomalies = np.argwhere(labels > threshold)\n",
    "                labels = np.zeros(labels.shape)\n",
    "                labels[true_anomalies] = 1\n",
    "\n",
    "            case \"points_score\":\n",
    "                # Computes the threshold using the percentiles\n",
    "                if threshold is None:\n",
    "                    mean = np.mean(point_scores)\n",
    "                    std = np.std(point_scores)\n",
    "                    a, b = (0 - mean) / std, (1 - mean) / std\n",
    "                    threshold = truncnorm.ppf(1 - self.anomaly_portion,\n",
    "                                              a,\n",
    "                                              b,\n",
    "                                              loc=mean,\n",
    "                                              scale=std)\n",
    "\n",
    "                labels[np.argwhere(point_scores > threshold)] = 1\n",
    "\n",
    "        return labels, threshold\n",
    "\n",
    "    def __check_parameters(self) -> None:\n",
    "        if self.scoring not in self.ACCEPTED_SCORING_METHODS:\n",
    "            raise ValueError(\"Scoring method must be one of the following: \" +\n",
    "                             str(self.ACCEPTED_SCORING_METHODS))\n",
    "        elif self.scaling not in self.ACCEPTED_SCALING_METHODS:\n",
    "            raise ValueError(\"Scoring method must be one of the following: \" +\n",
    "                             str(self.ACCEPTED_SCALING_METHODS))\n",
    "        elif self.classification not in self.ACCEPTED_LABELLING_METHODS:\n",
    "            raise ValueError(\"Scoring method must be one of the following: \" +\n",
    "                             str(self.ACCEPTED_LABELLING_METHODS))\n",
    "        elif self.window <= 0 or self.stride <= 0:\n",
    "            raise ValueError(\"Stride and window must be positive.\")\n",
    "        elif self.threshold is not None and not 0 <= self.threshold <= 1:\n",
    "            raise ValueError(\"Threshold must be None or 0 <= threshold <= 1\")\n",
    "        elif not 0 < self.anomaly_portion <= 0.5:\n",
    "            raise ValueError(\"The contamination must be inside (0,0.5]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b8d3e",
   "metadata": {},
   "source": [
    "# Wrapper of spatial models\n",
    "\n",
    "Every machine learning spatial algorithm must be wrapped in some sense. We do not re-implement existing methods of scikit-learn or other. We wrap these methods using the Adapter design pattern and by transforming univariate time series data into spatial data by the previously cited projection of the univariate time series onto $\\mathbb{R}^n$ where $n$ is the dimension of the window. At this point, we have that all models compute the anomaly score and labels with the same logic. Being that said, we implement these few methods which can again be shared by all implemented machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "025a5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "class TimeSeriesAnomalyWindowWrapper(TimeSeriesAnomalyWindow, ITimeSeriesAnomalyWrapper, ABC):\n",
    "    def __init__(self, window: int = 5,\n",
    "                 stride: int = 1,\n",
    "                 scaling: str = \"minmax\",\n",
    "                 scoring: str = \"average\",\n",
    "                 classification: str = \"voting\",\n",
    "                 threshold: float = None,\n",
    "                 anomaly_portion: float = 0.01):\n",
    "        super().__init__(window=window,\n",
    "                         stride=stride,\n",
    "                         scaling=scaling,\n",
    "                         scoring=scoring,\n",
    "                         classification=classification,\n",
    "                         threshold=threshold,\n",
    "                         anomaly_portion=anomaly_portion)\n",
    "        \n",
    "        self._wrapped_model = None\n",
    "        \n",
    "    def regress(self, x, *args, **kwargs) -> np.ndarray:\n",
    "        return self.anomaly_score(x)\n",
    "    \n",
    "    def anomaly_score(self, x, *args, **kwargs) -> np.ndarray:\n",
    "        # Input validation\n",
    "        check_array(x)\n",
    "        x = np.array(x)\n",
    "        \n",
    "        # Projects the time series onto a vector space\n",
    "        x_new, windows_per_point = self._project_time_series(x)\n",
    "        \n",
    "        # Get the window scores\n",
    "        window_scores = self._compute_window_scores(x_new)\n",
    "        anomaly_scores= self._compute_point_scores(window_scores,\n",
    "                                                   windows_per_point)\n",
    "        return anomaly_scores\n",
    "    \n",
    "    def classify(self, X, *args, **kwargs) -> np.ndarray:\n",
    "        # Input validation\n",
    "        check_array(X)\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Projects the time series onto a vector space\n",
    "        x_new, windows_per_point = self._project_time_series(X)\n",
    "        \n",
    "        # Get window labels\n",
    "        window_scores = self._compute_window_scores(x_new)\n",
    "        window_anomalies = self._compute_window_labels(x_new)\n",
    "        anomaly_scores = self._compute_point_scores(window_scores,\n",
    "                                                    windows_per_point)\n",
    "        labels, _ = self._compute_point_labels(window_anomalies,\n",
    "                                               windows_per_point,\n",
    "                                               anomaly_scores)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2ab59",
   "metadata": {},
   "source": [
    "# LOF for time series\n",
    "\n",
    "Between all spatial method, LOF is the easiest approach to apply to univariate time series data. It is specifically desifned to perform anomaly detection. It is able to compute anomalies and to give an abnormality score of each point. Therefore, the only thing we need to do, is to call the wrapped LOF method and use the scores and the labels it computes for the windows.\n",
    "\n",
    "# Unsupervised version\n",
    "\n",
    "LOF allows also to be implemented as novelty detection, whose approach is self-supervised. It only needs to set the novelty flag to true. In such a case, LOF must be trained on a training set and then tested. It must not be used on the same set since it will have no meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9facdfe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IParametric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalOutlierFactor\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTimeSeriesAnomalyLOF\u001b[39;00m(TimeSeriesAnomalyWindowWrapper, \u001b[43mIParametric\u001b[49m):\t\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, window: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      9\u001b[0m                  stride: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m                  scaling: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m                  novelty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m                  n_jobs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(window\u001b[38;5;241m=\u001b[39mwindow,\n\u001b[0;32m     25\u001b[0m                          stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m     26\u001b[0m                          scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                          threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[0;32m     30\u001b[0m                          anomaly_portion\u001b[38;5;241m=\u001b[39manomaly_portion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IParametric' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Union, Callable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "class TimeSeriesAnomalyLOF(TimeSeriesAnomalyWindowWrapper, IParametric):\n",
    "    def __init__(self, window: int = 5,\n",
    "                 stride: int = 1,\n",
    "                 scaling: str = \"minmax\",\n",
    "                 scoring: str = \"average\",\n",
    "                 classification: str = \"voting\",\n",
    "                 threshold: float = None,\n",
    "                 anomaly_portion: float = 0.01,\n",
    "                 n_neighbors: int = 20,\n",
    "                 algorithm: str = 'auto',\n",
    "                 leaf_size: int = 30,\n",
    "                 metric: Union[str, Callable[[list, list], float]] = 'minkowski',\n",
    "                 p: int = 2,\n",
    "                 metric_params: dict = None,\n",
    "                 contamination: Union[str, float] = 'auto',\n",
    "                 novelty: bool = False,\n",
    "                 n_jobs: int = None):\n",
    "        super().__init__(window=window,\n",
    "                         stride=stride,\n",
    "                         scaling=scaling,\n",
    "                         scoring=scoring,\n",
    "                         classification=classification,\n",
    "                         threshold=threshold,\n",
    "                         anomaly_portion=anomaly_portion)\n",
    "\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.metric_params = metric_params\n",
    "        self.contamination = contamination\n",
    "        self.novelty = novelty\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, x, y=None, *args, **kwargs) -> None:\n",
    "        check_array(x)\n",
    "        x = np.array(x)\n",
    "        \n",
    "        x_new, windows_per_point = self._project_time_series(x)\n",
    "        self._build_wrapped()\n",
    "        self._wrapped_model.fit(x_new)\n",
    "\n",
    "    def anomaly_score(self, x, *args, **kwargs) -> np.ndarray:\n",
    "        if self.novelty:\n",
    "            check_not_default_attributes(self, {\"_wrapped_model\": None})\n",
    "        return super().anomaly_score(x)\n",
    "    \n",
    "    def classify(self, X, *args, **kwargs) -> np.ndarray:\n",
    "        if self.novelty:\n",
    "            check_not_default_attributes(self, {\"_wrapped_model\": None})\n",
    "        return super().classify(X)\n",
    "    \n",
    "    def _compute_window_labels(self, vector_data: np.ndarray) -> np.ndarray:\n",
    "        # If the model is used as novelty it directly predicts\n",
    "        if self.novelty:\n",
    "            # Anomalies are -1 in LOF\n",
    "            window_anomalies = self._wrapped_model.predict(vector_data) * -1\n",
    "        else:\n",
    "            self._build_wrapped()\n",
    "            \n",
    "            # Anomalies are -1 in LOF\n",
    "            window_anomalies = self._wrapped_model.fit_predict(vector_data) * -1\n",
    "            \n",
    "        return window_anomalies\n",
    "\n",
    "    def _compute_window_scores(self, vector_data: np.ndarray) -> np.ndarray:\n",
    "        if self.novelty:\n",
    "            window_scores = - self._wrapped_model.decision_function(vector_data)\n",
    "        else:\n",
    "            self._build_wrapped()\n",
    "            \n",
    "            # I use fit since I do not need the labels\n",
    "            self._wrapped_model.fit(vector_data)\n",
    "            window_scores = - self._wrapped_model.negative_outlier_factor_\n",
    "        \n",
    "        return window_scores\n",
    "\n",
    "    def _build_wrapped(self) -> None:\n",
    "        self._wrapped_model = LocalOutlierFactor(self.n_neighbors,\n",
    "                                                 algorithm=self.algorithm,\n",
    "                                                 leaf_size=self.leaf_size,\n",
    "                                                 metric=self.metric,\n",
    "                                                 p=self.p,\n",
    "                                                 metric_params=self.metric_params,\n",
    "                                                 contamination=self.contamination,\n",
    "                                                 novelty=self.novelty,\n",
    "                                                 n_jobs=self.n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b951e6",
   "metadata": {},
   "source": [
    "# Metrics and plots\n",
    "\n",
    "To be able to evaluate models, we need to compute some metrics and to plot values of the metrics and of the predictions if we want to correctly asses the quality of the tested model. To do so, the vast majority of the metrics will be called from `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "def compute_metrics(true_labels: np.ndarray,\n",
    "                    scores: np.ndarray,\n",
    "                    pred_labels: np.ndarray = None,\n",
    "                    compute_roc_auc: bool = True,\n",
    "                    only_roc_auc: bool = False):\n",
    "    if not only_roc_auc:\n",
    "        precision = metrics.precision_score(true_labels, pred_labels)\n",
    "        recall = metrics.recall_score(true_labels, pred_labels)\n",
    "        f1_score = metrics.f1_score(true_labels, pred_labels)\n",
    "        accuracy = metrics.accuracy_score(true_labels, pred_labels)\n",
    "        avg_precision = metrics.average_precision_score(true_labels, scores)\n",
    "        pre, rec, _ = precision_recall_curve(true_labels, scores, pos_label=1)\n",
    "        precision_recall_auc = metrics.auc(rec, pre)\n",
    "        \n",
    "        print(\"ACCURACY SCORE: \", accuracy)\n",
    "        print(\"PRECISION SCORE: \", precision)\n",
    "        print(\"RECALL SCORE: \", recall)\n",
    "        print(\"F1 SCORE: \", f1_score)\n",
    "        print(\"AVERAGE PRECISION SCORE: \", avg_precision)\n",
    "        print(\"PRECISION-RECALL AUC SCORE: \", precision_recall_auc)\n",
    "    \n",
    "    if compute_roc_auc:\n",
    "        roc_auc = roc_auc_score(true_labels, scores)\n",
    "        print(\"AUROC SCORE: \", roc_auc)\n",
    "\n",
    "def plot_roc_curve(true_labels: np.ndarray,\n",
    "                   true_scores: np.ndarray,\n",
    "                   pos_label: int = 1,\n",
    "                   sample_weights: np.ndarray = None,\n",
    "                   drop_intermediate: bool = True,\n",
    "                   fig_size: Tuple = (6, 6),\n",
    "                   curve_color: str = 'b') -> None:\n",
    "    fpr, tpr, thresholds = roc_curve(true_labels,\n",
    "                                     true_scores,\n",
    "                                     pos_label=pos_label,\n",
    "                                     sample_weight=sample_weights,\n",
    "                                     drop_intermediate=drop_intermediate)\n",
    "    curve_fmt = curve_color + '-'\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    plt.plot([0, 1], [0, 1], 'k-', linewidth=0.5)\n",
    "    plt.plot(fpr, tpr, curve_fmt, linewidth=0.5)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"Fallout [FP / (FP + TN)]\")\n",
    "    plt.ylabel(\"Recall [TP / (FN + TP)]\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(true_labels: np.ndarray,\n",
    "                                scores: np.ndarray,\n",
    "                                pos_label: int = 1,\n",
    "                                sample_weights: np.ndarray = None,\n",
    "                                fig_size: Tuple = (6, 6),\n",
    "                                curve_color: str = 'b') -> None:\n",
    "    pre, rec, thresholds = precision_recall_curve(true_labels,\n",
    "                                                  scores,\n",
    "                                                  pos_label=pos_label,\n",
    "                                                  sample_weight=sample_weights)\n",
    "    curve_fmt = curve_color + '-'\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    plt.plot([0, 1], [1, 0], 'k-', linewidth=0.5)\n",
    "    plt.plot(rec, pre, curve_fmt, linewidth=0.5)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall [TP / (FN + TP)]\")\n",
    "    plt.ylabel(\"Precision [TP / (FP + TP)]\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix: np.ndarray,\n",
    "                          fig_size: Tuple = (6, 6)) -> None:\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    sn.heatmap(confusion_matrix, cmap=\"Blues\", annot=True, fmt=\".0f\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"True values\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_univariate_time_series_predictions(dataframe: pd.DataFrame,\n",
    "                                            predictions: np.ndarray,\n",
    "                                            index_column: str = \"timestamp\",\n",
    "                                            value_column: str = \"value\",\n",
    "                                            target_column: str = \"target\",\n",
    "                                            num_ticks: int = 5,\n",
    "                                            fig_size: Tuple = (16, 15),\n",
    "                                            data_color: str = 'k',\n",
    "                                            label_color: str = 'r',\n",
    "                                            pred_color: str = 'g') -> None:\n",
    "    \"\"\"Plot data with its labels and the relative predictions\"\"\"\n",
    "    fictitious_idx, indexes, ticks = __compute_idx_ticks(dataframe,\n",
    "                                                         num_ticks,\n",
    "                                                         index_column)\n",
    "    data_fmt, label_fmt, pred_fmt = __compute_formats(data_color,\n",
    "                                                      label_color,\n",
    "                                                      pred_color)\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=fig_size)\n",
    "    __plot_time_series_and_labels(dataframe,\n",
    "                                  data_fmt,\n",
    "                                  label_fmt,\n",
    "                                  fictitious_idx,\n",
    "                                  indexes,\n",
    "                                  ticks,\n",
    "                                  value_column,\n",
    "                                  target_column,\n",
    "                                  axs)\n",
    "\n",
    "    axs[2].plot(fictitious_idx,\n",
    "                predictions,\n",
    "                pred_fmt,\n",
    "                linewidth=0.5)\n",
    "    axs[2].set_xticks(indexes, ticks)\n",
    "    axs[2].set_title(\"Time series predictions\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_series_forecast(real_array: np.ndarray,\n",
    "                              pred_array: np.ndarray,\n",
    "                              num_ticks: int = 5,\n",
    "                              fig_size: Tuple = (16, 6),\n",
    "                              real_color: str = 'k',\n",
    "                              pred_color: str = 'g',\n",
    "                              on_same_plot: bool = True) -> None:\n",
    "    fictitious_idx = np.arange(real_array.shape[0])\n",
    "    indexes = np.linspace(0, real_array.shape[0] - 1, num_ticks, dtype=np.intc)\n",
    "    ticks = indexes\n",
    "    real_data_fmt = real_color + '-'\n",
    "    pred_data_fmt = pred_color + '-'\n",
    "    \n",
    "    if on_same_plot:\n",
    "        fig = plt.figure(figsize=fig_size)\n",
    "        plt.plot(fictitious_idx,\n",
    "                 real_array,\n",
    "                 real_data_fmt,\n",
    "                 linewidth=1)\n",
    "        plt.plot(fictitious_idx,\n",
    "                 pred_array,\n",
    "                 pred_data_fmt,\n",
    "                 linewidth=1)\n",
    "        plt.xticks(indexes, ticks)\n",
    "        plt.title(\"Time series data\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, axs = plt.subplots(2, 1, figsize=fig_size)\n",
    "        axs[0].plot(fictitious_idx,\n",
    "                 real_array,\n",
    "                 real_data_fmt,\n",
    "                 linewidth=1)\n",
    "        axs[0].set_xticks(indexes, ticks)\n",
    "        axs[0].set_title(\"Time series data\")\n",
    "        \n",
    "        axs[1].plot(fictitious_idx,\n",
    "                 pred_array,\n",
    "                 pred_data_fmt,\n",
    "                 linewidth=1)\n",
    "        axs[1].set_xticks(indexes, ticks)\n",
    "        axs[1].set_title(\"Time series predictions\")\n",
    "        plt.show()\n",
    "\n",
    "def plot_time_series_with_predicitons_bars(dataframe: pd.DataFrame,\n",
    "                                           predictions: np.ndarray,\n",
    "                                           bars: np.ndarray,\n",
    "                                           index_column: str = \"timestamp\",\n",
    "                                           value_column: str = \"value\",\n",
    "                                           num_ticks: int = 5,\n",
    "                                           fig_size: Tuple = (16, 5),\n",
    "                                           data_color: str = 'k',\n",
    "                                           pred_color: str = 'g') -> None:\n",
    "    fictitious_idx, indexes, ticks = __compute_idx_ticks(dataframe,\n",
    "                                                         num_ticks,\n",
    "                                                         index_column)\n",
    "    data_fmt, _, pred_fmt = __compute_formats(data_color,\n",
    "                                              pred_color=pred_color,\n",
    "                                              pred_type=\"point\")\n",
    "    anomalies = np.argwhere(predictions == 1)\n",
    "\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    plt.plot(fictitious_idx,\n",
    "             dataframe[value_column],\n",
    "             data_fmt,\n",
    "             linewidth=0.5)\n",
    "    plt.scatter(fictitious_idx[anomalies],\n",
    "                np.array(dataframe[value_column])[anomalies],\n",
    "                c=pred_fmt)\n",
    "    plt.vlines(fictitious_idx[bars],\n",
    "               np.min(np.array(dataframe[value_column])),\n",
    "               np.max(np.array(dataframe[value_column])),\n",
    "               color=['r']*(fictitious_idx[bars]).size)\n",
    "    plt.xticks(indexes, ticks)\n",
    "    plt.title(\"Time series with predictions as dots\")\n",
    "    plt.show()\n",
    "\n",
    "def get_windows_indices(dataframe: pd.DataFrame,\n",
    "                        data_key: str,\n",
    "                        combined_windows_path: str,\n",
    "                        index_column: str = \"timestamp\") -> np.ndarray:\n",
    "    file = open(combined_windows_path)\n",
    "    combined_windows = json.load(file)\n",
    "    file.close()\n",
    "\n",
    "    desired_windows = combined_windows[data_key]\n",
    "    desired_windows = [elem for window in desired_windows for elem in window]\n",
    "    desired_windows = [elem[:-7] for elem in desired_windows]\n",
    "    timestamps = dataframe[index_column]\n",
    "    mask = np.zeros(np.array(dataframe[index_column]).size)\n",
    "    for i in range(np.array(timestamps).size):\n",
    "        if timestamps[i] in desired_windows:\n",
    "            mask[i] = 1\n",
    "    bool_mask = [True if val == 1 else False for val in mask]\n",
    "\n",
    "    return np.argwhere(bool_mask)\n",
    "\n",
    "\n",
    "def get_bars_indices_on_test_df(complete_df: pd.DataFrame,\n",
    "                                dataframe: pd.DataFrame,\n",
    "                                data_key: str,\n",
    "                                combined_windows_path: str,\n",
    "                                index_column: str = \"timestamp\") -> np.array:\n",
    "    bars = get_windows_indices(complete_df, data_key, combined_windows_path, index_column)\n",
    "    all_timestamps = complete_df[\"timestamp\"].tolist()\n",
    "    bars = [dataframe[\"timestamp\"].tolist().index(all_timestamps[int(bar)])\n",
    "            for bar in bars\n",
    "            if all_timestamps[int(bar)] in dataframe[\"timestamp\"].tolist()]\n",
    "    return np.array(bars)\n",
    "\n",
    "def __compute_idx_ticks(dataframe: pd.DataFrame,\n",
    "                        num_ticks: int,\n",
    "                        index_column: str):\n",
    "    fictitious_idx = np.arange(dataframe.shape[0])\n",
    "    indexes = np.linspace(0, dataframe.shape[0] - 1, num_ticks, dtype=np.intc)\n",
    "    ticks = np.array(dataframe[index_column])[indexes] if index_column is not None else indexes\n",
    "    return fictitious_idx, indexes, ticks\n",
    "\n",
    "\n",
    "def __compute_formats(data_color: str,\n",
    "                      label_color: str = None,\n",
    "                      pred_color: str = 'g',\n",
    "                      data_type: str = \"line\",\n",
    "                      label_type: str = \"line\",\n",
    "                      pred_type: str = \"line\") -> Tuple:\n",
    "    d_sep = '-' if data_type == \"line\" else ''\n",
    "    l_sep = '-' if label_type == \"line\" else ''\n",
    "    p_sep = '-' if pred_type == \"line\" else ''\n",
    "    data_fmt = data_color + d_sep\n",
    "    label_fmt = label_color + l_sep if label_color is not None else None\n",
    "    pred_fmt = pred_color + p_sep\n",
    "    return data_fmt, label_fmt, pred_fmt\n",
    "\n",
    "\n",
    "def __plot_time_series_and_labels(dataframe: pd.DataFrame,\n",
    "                                  data_fmt: str,\n",
    "                                  label_fmt: str,\n",
    "                                  fictitious_idx: np.ndarray,\n",
    "                                  indexes: np.ndarray,\n",
    "                                  ticks: list,\n",
    "                                  value_column: str,\n",
    "                                  target_column: str,\n",
    "                                  axs) -> None:\n",
    "    axs[0].plot(fictitious_idx,\n",
    "                dataframe[value_column],\n",
    "                data_fmt,\n",
    "                linewidth=0.5)\n",
    "    axs[0].set_xticks(indexes, ticks)\n",
    "    axs[0].set_title(\"Time series data\")\n",
    "\n",
    "    axs[1].plot(fictitious_idx,\n",
    "                dataframe[target_column],\n",
    "                label_fmt,\n",
    "                linewidth=0.5)\n",
    "    axs[1].set_xticks(indexes, ticks)\n",
    "    axs[1].set_title(\"Time series labels\")\n",
    "\n",
    "def make_metric_plots(dataframe: pd.DataFrame,\n",
    "                      true_labels: np.ndarray,\n",
    "                      scores: np.ndarray,\n",
    "                      pred_labels: np.ndarray):\n",
    "    confusion_matrix = metrics.confusion_matrix(true_labels, pred_labels)\n",
    "    plot_roc_curve(true_labels, scores)\n",
    "    plot_precision_recall_curve(true_labels, scores)\n",
    "    plot_confusion_matrix(confusion_matrix)\n",
    "    plot_univariate_time_series_predictions(dataframe, pred_labels)\n",
    "    plot_univariate_time_series_predictions(dataframe, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b56c8",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "\n",
    "Given the previous utility functions to read data and the model to perform anomaly detection, we can start by reading the dataset. Then, we preprocess data to be able to identify anomalies with the unsupervised approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATASET_PATH = \"../data/dataset/\"\n",
    "DATASET = \"nyc_taxi.csv\"\n",
    "PURE_DATA_KEY = \"realKnownCause/nyc_taxi.csv\"\n",
    "GROUND_WINDOWS_PATH = \"../data/dataset/combined_windows.json\"\n",
    "\n",
    "reader = NABTimeSeriesReader(DATASET_PATH)\n",
    "all_df = reader.read(DATASET_PATH + DATASET).get_dataframe()\n",
    "\n",
    "def preprocess(X) -> np.ndarray:\n",
    "    return StandardScaler().fit_transform(X)\n",
    "\n",
    "data_test = preprocess(np.array(all_df[\"value\"]).reshape(all_df[\"value\"].shape[0], 1))\n",
    "data_test_labels = all_df[\"target\"]\n",
    "dataframe = all_df.copy()\n",
    "dataframe[\"value\"] = data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5fc3f",
   "metadata": {},
   "source": [
    "# Get labels and scores for points\n",
    "\n",
    "To be able to get scores and labels with unsupervised approach, given all the previous architecture, we can simply declare the model and call the `classify` function to get the labels for each point and we call `anomaly_score` to get the anomaly score of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesAnomalyLOF(window=3,\n",
    "                             classification=\"points_score\",\n",
    "                             anomaly_portion=0.01,\n",
    "                             n_neighbors=100)\n",
    "\n",
    "labels = model.classify(data_test.reshape((-1, 1)))\n",
    "scores = model.anomaly_score(data_test.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd2f6f",
   "metadata": {},
   "source": [
    "# Compute metrics for evaluation\n",
    "\n",
    "To evaluate the model, metrics must be computed. To be complete in the metrics computation, we will compute the following metrics: accuracy, precision, recall, F1, average precision score, precision recall AUC and ROC AUC. Of each of these metrics we can even compute graphics to visualize the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = data_test_labels\n",
    "compute_metrics(true_labels, scores, labels, only_roc_auc=False)\n",
    "make_metric_plots(dataframe, true_labels, scores, labels)\n",
    "\n",
    "bars = get_bars_indices_on_test_df(all_df,\n",
    "                                   dataframe,\n",
    "                                   PURE_DATA_KEY,\n",
    "                                   GROUND_WINDOWS_PATH)\n",
    "plot_time_series_with_predicitons_bars(dataframe,\n",
    "                                       labels,\n",
    "                                       bars,\n",
    "                                       pred_color='r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
